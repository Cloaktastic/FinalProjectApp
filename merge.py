
#Import All the Required Libraries
import cv2
import streamlit as st
from ultralytics import YOLO
from PIL import Image
import numpy as np
import os
import subprocess
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
import requests
import time
import av
from collections import deque

#Sources
IMAGE = 'Image'
VIDEO = 'Video'
CAMERA = 'Camera'
SOURCES_LIST = [IMAGE, VIDEO, CAMERA]

VIDEO_DIR = 'App/videos'
VIDEOS_DICT = {
    'video 1': VIDEO_DIR + '/' + '01.mp4',
    'video 2': VIDEO_DIR + '/' + '02.mp4',
    'video 3': VIDEO_DIR + '/' + '03.mp4',
}

#Page Layout
st.set_page_config(
    page_title = "Elderly Fall Detection - Computer Vision Approach",
    page_icon = "üö®"
)

#Header
st.header("YOLOv8n for Fall & Person Detection üöëüë§")

#SideBar
st.sidebar.header("Model Configurations")

confidence_value = float(st.sidebar.slider("Select Model Confidence Value", 25, 100, 40))/100

st.sidebar.header("Mode")

source_radio = st.sidebar.radio(
    "Select Source", SOURCES_LIST
)

#Load the YOLO Model
model_path = 'App/models/100_epochs.pt'  # This model can detect both fall and person
try:
    model = YOLO(model_path)
    # Get class names to understand what the model detects
    class_names = model.names
except Exception as e:
    st.error(f"Unable to load model. Check the sepcified path: {model_path}")
    st.error(e)

#Telegram functions
def tg_send_message(token, chat_id, text):
    try:
        response = requests.post(
            f"https://api.telegram.org/bot{token}/sendMessage",
            data={"chat_id": chat_id, "text": text},
            timeout=15
        )
        if response.status_code != 200:
            return False
        else:
            return True
    except Exception as e:
        print("Telegram message failed:", e)
        return False

def video_callback(frame):
    img = frame.to_ndarray(format="bgr24")

    # YOLO predict
    results = model(img, verbose=False)[0]
    boxes = results.boxes

    # V·∫Ω bounding boxes cho c·∫£ fall v√† person
    if boxes is not None and len(boxes) > 0:
        for box in boxes:
            # L·∫•y t·ªça ƒë·ªô bounding box
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            
            # L·∫•y class ID v√† t√™n class
            class_id = int(box.cls[0])
            class_name = model.names[class_id]
            conf = box.conf[0].cpu().numpy()
            
            # X√°c ƒë·ªãnh m√†u v√† label d·ª±a tr√™n class
            if class_name.lower() == 'fall':
                # Fall class - ƒë·ªè
                box_color = (0, 0, 255)
                label_text = f"FALL: {conf:.2f}"
                print(f"Fall detected! Confidence: {conf:.3f}")
            elif class_name.lower() in ['person', 'people', 'human']:
                # Person class - xanh l√°
                box_color = (0, 255, 0)
                label_text = f"PERSON: {conf:.2f}"
                print(f"Person detected! Confidence: {conf:.3f}")
            else:
                # Class kh√°c - v√†ng
                box_color = (0, 255, 255)
                label_text = f"{class_name.upper()}: {conf:.2f}"
            
            # V·∫Ω rectangle
            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), box_color, 2)
            
            # V·∫Ω label v·ªõi confidence
            cv2.putText(img, label_text,
                       (int(x1), int(y1) - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)

    # Tr·∫£ frame cho WebRTC hi·ªÉn th·ªã
    return av.VideoFrame.from_ndarray(img, format="bgr24")

def tg_send_video(token, chat_id, path, caption=""):
    try:
        with open(path, "rb") as f:
            response = requests.post(
                f"https://api.telegram.org/bot{token}/sendVideo",
                data={"chat_id": chat_id, "caption": caption},
                files={"video": (os.path.basename(path), f, "video/mp4")},
                timeout=180
            )
        if response.status_code != 200:
            print("Video error response:", response.text)
        else:
            print("Video sent successfully")
    except Exception as e:
        print("Telegram video failed:", e)

def tg_send_audio(token, chat_id, path, caption=""):
    try:
        with open(path, "rb") as f:
            response = requests.post(
                f"https://api.telegram.org/bot{token}/sendAudio",
                data={"chat_id": chat_id, "caption": caption},
                files={"audio": (os.path.basename(path), f, "audio/ogg")},
                timeout=180
            )
        if response.status_code != 200:
            print("Audio error response:", response.text)
        else:
            print("Audio sent successfully")
    except Exception as e:
        print("Telegram audio failed:", e)



class VideoTransformer(VideoTransformerBase):
    def __init__(self, tg_token, tg_chat):
        self.tg_token = tg_token
        self.tg_chat = tg_chat
        self.buf = deque(maxlen=40)  # Buffer for storing frames
        self.rec_fps = 3.0
        self.pre_sec = 15.0
        self.post_sec = 15.0

    def transform(self, frame):
        img = frame.to_ndarray(format="bgr")
        t_now = time.time()
        self.buf.append((t_now, img.copy()))

        # Detect falls (using YOLO model)
        results = model(img, verbose=False)[0]
        boxes = results.boxes
        
        # Check for both fall and person detections
        fall_detected = False
        if boxes:
            for box in boxes:
                class_id = int(box.cls[0])
                class_name = model.names[class_id]
                if class_name.lower() == 'fall':
                    fall_detected = True
                    break
        
        if fall_detected:  # If a fall is detected
            print("Fall detected!")
            self.ensure_and_save_30s_clip(t_now)

        return img

    def ensure_and_save_30s_clip(self, t0):
        # Wait until enough frames are collected
        deadline = t0 + self.post_sec
        while True:
            latest_t = self.buf[-1][0] if self.buf else 0.0
            if latest_t >= deadline:
                break
            time.sleep(0.2)

        frames = [f for (t, f) in list(self.buf) if (t >= t0 - self.pre_sec and t <= t0 + self.post_sec)]
        if not frames:
            return None

        # Save video
        tmp_path = f"videos/face_{int(time.time())}.mp4"
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        vw = cv2.VideoWriter(tmp_path, fourcc, self.rec_fps, (frames[0][1].shape[1], frames[0][1].shape[0]))
        for f in frames:
            vw.write(f[1])
        vw.release()

        # Send video to Telegram
        tg_send_video(self.tg_token, self.tg_chat, tmp_path, caption="Fall detected!")

source_image = None
if source_radio == IMAGE:
    source_image = st.sidebar.file_uploader(
        "Choose an Image....", type = ("jpg", "png", "jpeg", "bmp", "webp")
    )
    col1, col2 = st.columns(2)
    with col1:
        # try:
        #     uploaded_image  =Image.open(source_image)
        #     st.image(source_image, caption = "Uploaded Image", use_container_width = True)
        # except Exception as e:
        #     st.error("Error Occured While Opening the Image")
        #     st.error(e)
        if source_image: 
          uploaded_image  =Image.open(source_image)
          st.image(source_image, caption = "Uploaded Image", use_container_width = True)
        else:
          st.text("Please Upload an Image")
    with col2:
        try:
            if st.sidebar.button("Detect Objects"):
                result = model.predict(uploaded_image, conf = confidence_value)
                boxes = result[0].boxes
                
                # V·∫Ω bounding boxes th·ªß c√¥ng ƒë·ªÉ c√≥ m√†u s·∫Øc ph√¢n bi·ªát
                img_array = cv2.cvtColor(np.array(uploaded_image), cv2.COLOR_RGB2BGR)
                
                if boxes is not None and len(boxes) > 0:
                    fall_count = 0
                    person_count = 0
                    
                    for box in boxes:
                        # L·∫•y t·ªça ƒë·ªô bounding box
                        x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())
                        
                        # L·∫•y class ID v√† t√™n class
                        class_id = int(box.cls[0])
                        class_name = model.names[class_id]
                        conf = box.conf[0].cpu().numpy()
                        
                        # X√°c ƒë·ªãnh m√†u v√† label d·ª±a tr√™n class
                        if class_name.lower() == 'fall':
                            # Fall class - ƒë·ªè
                            box_color = (0, 0, 255)
                            label_text = f"FALL: {conf:.2f}"
                            fall_count += 1
                        elif class_name.lower() in ['person', 'people', 'human']:
                            # Person class - xanh l√°
                            box_color = (0, 255, 0)
                            label_text = f"PERSON: {conf:.2f}"
                            person_count += 1
                        else:
                            # Class kh√°c - v√†ng
                            box_color = (0, 255, 255)
                            label_text = f"{class_name.upper()}: {conf:.2f}"
                        
                        # V·∫Ω rectangle
                        cv2.rectangle(img_array, (x1, y1), (x2, y2), box_color, 2)
                        
                        # V·∫Ω label v·ªõi confidence
                        cv2.putText(img_array, label_text,
                                   (x1, y1 - 10),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)
                    
                    # Th√™m text t·ªïng k·∫øt
                    summary_text = f"Detection Summary: {person_count} person(s), {fall_count} fall(s)"
                    cv2.putText(img_array, summary_text,
                               (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                else:
                    cv2.putText(img_array, "No objects detected",
                               (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
                
                # Convert back to RGB for display
                result_plotted = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
                st.image(result_plotted, caption = "Detected Image", use_container_width = True)

                try:
                    with st.expander("Detection Results"):
                        if boxes is not None and len(boxes) > 0:
                            st.write("**Detections found:**")
                            for i, box in enumerate(boxes):
                                class_id = int(box.cls[0])
                                class_name = model.names[class_id]
                                conf = box.conf[0].cpu().numpy()
                                coords = box.xyxy[0].cpu().numpy()
                                st.write(f"{i+1}. {class_name} - Confidence: {conf:.3f} - Coordinates: {coords}")
                        else:
                            st.write("No objects detected in the image.")
                except Exception as e:
                    st.error(e)
        except Exception as e:
            st.error("Error Occured While Opening the Image")
            st.error(e)

elif source_radio == VIDEO:
    source_video = st.sidebar.selectbox(
        "Choose a Video...", VIDEOS_DICT.keys()
    )
    with open(VIDEOS_DICT.get(source_video), 'rb') as video_file:
        video_bytes = video_file.read()
        if video_bytes:
            st.video(video_bytes)
        if st.sidebar.button("Detect Video Objects"):
            try:
                # Predict every frame of the video
                results = model(VIDEOS_DICT.get(source_video), save=True, show=True, conf = confidence_value)

                # Get the latest avi file
                DETECT_FOLDER = 'runs/detect'
                number_of_predictions = len(os.listdir(DETECT_FOLDER))
                if number_of_predictions == 1:
                  latest_predict = 'predict'
                else:
                  latest_predict = 'predict' + str(number_of_predictions)
                avi_file =  DETECT_FOLDER + '/' + latest_predict + '/' + os.listdir(DETECT_FOLDER + '/' + latest_predict)[0]

                # Convert from avi to mp4, as streamlit cannot view the video from the avi file
                output_mp4 = f"{latest_predict}.mp4"
                subprocess.run([
                                "ffmpeg", "-i", avi_file,
                                "-ac", "2", "-b:v", "2000k",
                                "-c:a", "aac", "-c:v", "libx264",
                                "-b:a", "160k", "-vprofile", "high",
                                "-bf", "0", "-strict", "experimental",
                                "-f", "mp4", output_mp4
                            ], check=True)

                # Show the video
                video_file = open(output_mp4, 'rb')
                video_bytes = video_file.read()
                st.video(video_bytes)

            except Exception as e:
                st.sidebar.error("Error Loading Video"+str(e))

# Camera section
elif source_radio == CAMERA:
    # Load Telegram token v√† chat ID
    with open('App/token.txt', 'r') as file:
        lines = file.readlines()
        tg_token = lines[0].strip()
        tg_chat = lines[1].strip()

    st.header("Camera Stream")
    st.write(
        "üìπ H·ªá th·ªëng s·∫Ω detect **t√© ng√£** v√† **ng∆∞·ªùi**.\n\n"
        "- V·∫Ω bounding box cho c·∫£ **person** (xanh l√°) v√† **fall** (ƒë·ªè).\n"
        "- Ph√°t hi·ªán t√© ng√£ khi **ng∆∞·ªùi n·∫±m b·∫•t ƒë·ªông 5 gi√¢y li√™n ti·∫øp**.\n"
        "- Ghi l·∫°i **30s video** (15s tr∆∞·ªõc + 15s sau th·ªùi ƒëi·ªÉm ph√°t hi·ªán).\n"
        "- G·ª≠i video l√™n Telegram ·ªü ƒë·ªãnh d·∫°ng xem ƒë∆∞·ª£c tr√™n Telegram Web."
    )

    # Kh·ªüi t·∫°o state l·∫ßn ƒë·∫ßu
    if "fall_state" not in st.session_state:
        st.session_state.fall_state = {
            "buf": deque(maxlen=900),   # ƒë·ªß ~30s n·∫øu fps ~30 (30*30 = 900)
            "detected": False,          # ƒë√£ detect t√© ng√£ ch∆∞a
            "saved": False,             # ƒë√£ l∆∞u/g·ª≠i video ch∆∞a
            "t0": None,                 # th·ªùi ƒëi·ªÉm ph√°t hi·ªán fall
            "pre_sec": 15.0,
            "post_sec": 15.0,
            "last_t": None,             # d√πng ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng fps
            "fps_sum": 0.0,
            "fps_n": 0,
            # Th√™m c√°c bi·∫øn ƒë·ªÉ t·ªëi ∆∞u bounding box
            "frame_count": 0,           # ƒë·∫øm s·ªë frame ƒë√£ x·ª≠ l√Ω
            "last_detection": None,     # l∆∞u k·∫øt qu·∫£ detect cu·ªëi
            "last_detection_time": 0,   # th·ªùi ƒëi·ªÉm detect cu·ªëi
            "detection_interval": 0.05,  # ch·ªâ detect m·ªói 0.05 gi√¢y (20 l·∫ßn/gi√¢y)
            "last_boxes": None,         # l∆∞u boxes cu·ªëi ƒë·ªÉ v·∫Ω
            # Th√™m c√°c bi·∫øn ƒë·ªÉ tracking motionless detection
            "motionless_start": None,   # th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu detect motionlessness
            "motionless_duration": 5.0, # c·∫ßn motionless trong 5 gi√¢y
            "is_motionless": False,     # ƒë√£ detect motionlessness ch∆∞a
            "continuous_fall_count": 0, # ƒë·∫øm s·ªë l·∫ßn detect li√™n ti·∫øp
        }

    state = st.session_state.fall_state

    def video_callback(frame):
        img = frame.to_ndarray(format="bgr24")
        t_now = time.time()

        # --- ∆Ø·ªöC L∆Ø·ª¢NG FPS TH·ª∞C T·∫æ ---
        if state["last_t"] is not None:
            dt = t_now - state["last_t"]
            if 0.005 < dt < 1.0:  # lo·∫°i b·ªè spike b·∫•t th∆∞·ªùng
                fps_inst = 1.0 / dt
                state["fps_sum"] += fps_inst
                state["fps_n"] += 1
        state["last_t"] = t_now

        # L∆∞u frame v√†o buffer (lu√¥n lu√¥n l∆∞u)
        state["buf"].append((t_now, img.copy()))

        # --- T·ªêI ∆ØU: CH·ªà DETECT M·ªñI 0.05 GI√ÇY ---
        should_detect = False
        if not state["detected"]:
            # Ch·ªâ detect n·∫øu ƒë·ªß th·ªùi gian interval ho·∫∑c ch∆∞a c√≥ k·∫øt qu·∫£ n√†o
            if (t_now - state["last_detection_time"]) >= state["detection_interval"]:
                should_detect = True
        
        # L∆∞u k·∫øt qu·∫£ detection ƒë·ªÉ v·∫Ω bounding box
        current_boxes = state["last_boxes"]
        
        if should_detect:
            # d√πng confidence_value t·ª´ sidebar
            results = model(img, verbose=False, conf=confidence_value)[0]
            boxes = results.boxes

            # L∆∞u k·∫øt qu·∫£ detection
            state["last_detection_time"] = t_now
            state["last_boxes"] = boxes
            
            # Ph√¢n lo·∫°i v√† x·ª≠ l√Ω detection results
            fall_detected = False
            person_detected = False
            
            if boxes is not None and len(boxes) > 0:
                for box in boxes:
                    # L·∫•y class ID v√† t√™n class
                    class_id = int(box.cls[0])
                    class_name = model.names[class_id]
                    
                    if class_name.lower() == 'fall':
                        fall_detected = True
                        print(f"Fall detected! Confidence: {box.conf[0]:.3f}")
                    elif class_name.lower() in ['person', 'people', 'human']:
                        person_detected = True
                        print(f"Person detected! Confidence: {box.conf[0]:.3f}")
            
            # Logic x·ª≠ l√Ω fall detection (gi·ªØ nguy√™n logic c≈©)
            if fall_detected:
                # B·∫Øt ƒë·∫ßu tracking th·ªùi gian motionless n·∫øu ch∆∞a b·∫Øt ƒë·∫ßu
                if state["motionless_start"] is None:
                    state["motionless_start"] = t_now
                    state["continuous_fall_count"] = 1
                else:
                    # TƒÉng s·ªë l·∫ßn detect li√™n ti·∫øp
                    state["continuous_fall_count"] += 1
                
                # Ki·ªÉm tra xem ƒë√£ ƒë·ªß 5 gi√¢y motionless ch∆∞a
                motionless_duration = t_now - state["motionless_start"]
                if motionless_duration >= state["motionless_duration"]:
                    print(f"CONFIRMED FALL! Motionless for {motionless_duration:.2f} seconds")
                    state["detected"] = True
                    state["t0"] = t_now
                    
                    # Send message to group
                    group_message = "‚ö†Ô∏è FALL CONFIRMED! Person motionless for 5 seconds. Recording 30-second clip (15s before + 15s after)..."
                    tg_send_message(tg_token, tg_chat, group_message)
                    

                    
                    # Reset tracking variables
                    state["motionless_start"] = None
                    state["continuous_fall_count"] = 0
            else:
                # N·∫øu kh√¥ng detect fall, reset tracking
                if state["motionless_start"] is not None:
                    print("Movement detected - resetting motionless timer")
                    state["motionless_start"] = None
                    state["continuous_fall_count"] = 0
            
            # Detection summary logged
            
            current_boxes = boxes

        # --- V·∫º BOUNDING BOX T·ª™ K·∫æT QU·∫¢ ƒê√É L∆ØU ---
        if current_boxes is not None and len(current_boxes) > 0:
            # V·∫Ω bounding boxes l√™n frame hi·ªán t·∫°i
            for box in current_boxes:
                # L·∫•y t·ªça ƒë·ªô bounding box
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                
                # L·∫•y class ID v√† t√™n class
                class_id = int(box.cls[0])
                class_name = model.names[class_id]
                conf = box.conf[0].cpu().numpy()
                
                # X√°c ƒë·ªãnh m√†u v√† label d·ª±a tr√™n class v√† tr·∫°ng th√°i
                if class_name.lower() == 'fall':
                    # Fall class - ƒë·ªè khi ch∆∞a confirm, ƒë·ªè ƒë·∫≠m khi ƒë√£ confirm
                    if state["detected"]:
                        box_color = (0, 0, 255)  # ƒê·ªè ƒë·∫≠m cho confirmed fall
                        label_text = f"FALL CONFIRMED: {conf:.2f}"
                    else:
                        box_color = (0, 0, 255)  # ƒê·ªè nh·∫°t cho fall detected
                        label_text = f"FALL DETECTED: {conf:.2f}"
                elif class_name.lower() in ['person', 'people', 'human']:
                    # Person class - xanh l√° ho·∫∑c xanh d∆∞∆°ng
                    box_color = (0, 255, 0)  # Xanh l√° cho person
                    label_text = f"PERSON: {conf:.2f}"
                else:
                    # Class kh√°c - v√†ng
                    box_color = (0, 255, 255)  # V√†ng/xanh d∆∞∆°ng
                    label_text = f"{class_name.upper()}: {conf:.2f}"
                
                # V·∫Ω rectangle v·ªõi m√†u t∆∞∆°ng ·ª©ng
                cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), box_color, 2)
                
                # V·∫Ω label v·ªõi confidence
                cv2.putText(img, label_text,
                           (int(x1), int(y1) - 10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)
                
                # Hi·ªÉn th·ªã th·ªùi gian motionless cho fall detection
                if class_name.lower() == 'fall' and state["motionless_start"] is not None and not state["detected"]:
                    motionless_time = t_now - state["motionless_start"]
                    remaining_time = max(0, state["motionless_duration"] - motionless_time)
                    timer_text = f"Motionless: {motionless_time:.1f}s (Wait {remaining_time:.1f}s)"
                    
                    # V·∫Ω timer l√™n frame
                    cv2.putText(img, timer_text,
                               (int(x1), int(y2) + 25),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    
                    # V·∫Ω v√≤ng tr√≤n progress
                    progress = min(1.0, motionless_time / state["motionless_duration"])
                    center_x, center_y = int(x2) + 30, int(y1) + 30
                    cv2.circle(img, (center_x, center_y), 15, (50, 50, 50), 2)
                    cv2.circle(img, (center_x, center_y), 15, (0, 255, 255), int(15 * progress), -1)
                
                # N·∫øu ƒë√£ confirmed fall, hi·ªÉn th·ªã th√¥ng b√°o ƒë·ªè l·ªõn tr√™n frame
                if state["detected"]:
                    cv2.putText(img, "‚ö†Ô∏è FALL CONFIRMED ‚ö†Ô∏è",
                               (50, 50),
                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
            
            # Hi·ªÉn th·ªã t·ªïng quan detection ·ªü g√≥c tr√™n b√™n ph·∫£i
            detection_counts = {"fall": 0, "person": 0, "other": 0}
            for box in current_boxes:
                class_id = int(box.cls[0])
                class_name = model.names[class_id]
                if class_name.lower() == 'fall':
                    detection_counts["fall"] += 1
                elif class_name.lower() in ['person', 'people', 'human']:
                    detection_counts["person"] += 1
                else:
                    detection_counts["other"] += 1
            
            # V·∫Ω summary box
            summary_text = f"Detections: {detection_counts['person']} person(s), {detection_counts['fall']} fall(s)"
            if detection_counts["other"] > 0:
                summary_text += f", {detection_counts['other']} other(s)"
            
            # Background cho summary
            text_size = cv2.getTextSize(summary_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(img, (img.shape[1] - text_size[0] - 20, 10), 
                         (img.shape[1] - 10, 40), (0, 0, 0), -1)
            cv2.putText(img, summary_text,
                       (img.shape[1] - text_size[0] - 10, 35),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # --- SAU KHI DETECT: CH·ªú ƒê·ª¶ 15s SAU R·ªíI L∆ØU CLIP ---
        if state["detected"] and not state["saved"]:
            t0 = state["t0"]
            pre_sec = state["pre_sec"]
            post_sec = state["post_sec"]

            if t_now >= t0 + post_sec:
                frames_all = list(state["buf"])

                # Gi·ªØ c·∫£ (t, f) ƒë·ªÉ t√≠nh FPS ch√≠nh x√°c cho ƒëo·∫°n clip
                selected = [
                    (t, f) for (t, f) in frames_all
                    if (t >= t0 - pre_sec and t <= t0 + post_sec)
                ]

                if selected:
                    # Th·ªùi gian th·ª±c c·ªßa ƒëo·∫°n clip
                    t_first = selected[0][0]
                    t_last = selected[-1][0]
                    duration = max(0.001, t_last - t_first)  # tr√°nh chia 0
                    n_frames = len(selected)

                    # FPS = s·ªë frame / th·ªùi gian
                    fps_clip = (n_frames - 1) / duration if n_frames > 1 else 10.0

                    # Gi·ªõi h·∫°n FPS cho m∆∞·ª£t, tr√°nh qu√° nhanh/ch·∫≠m
                    fps_clip = max(5.0, min(20.0, fps_clip))

                    print(f"Clip duration ~ {duration:.2f}s, frames = {n_frames}, fps_clip = {fps_clip:.2f}")

                    h, w = selected[0][1].shape[:2]
                    os.makedirs("videos", exist_ok=True)

                    raw_path = f"videos/fall_{int(time.time())}_raw.mp4"

                    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
                    vw = cv2.VideoWriter(raw_path, fourcc, fps_clip, (w, h))
                    for _, frame_img in selected:
                        vw.write(frame_img)
                    vw.release()

                    # --- Convert sang H.264 chu·∫©n Telegram Web ---
                    fixed_path = raw_path.replace("_raw.mp4", ".mp4")
                    cmd = [
                        "ffmpeg", "-y",
                        "-i", raw_path,
                        "-vcodec", "libx264",
                        "-pix_fmt", "yuv420p",
                        "-profile:v", "baseline",
                        "-level", "3.1",
                        "-movflags", "+faststart",
                        "-an",  # kh√¥ng audio
                        fixed_path
                    ]
                    subprocess.run(
                        cmd,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE
                    )

                    print(f"Saved fall clip -> {fixed_path}")
                    
                    # Send video to group
                    tg_send_video(
                        tg_token,
                        tg_chat,
                        fixed_path,
                        caption="üìπ Fall detected! 30-second clip (15s before + 15s after)."
                    )



                    state["saved"] = True
                    
                    # Send completion messages
                    tg_send_message(tg_token, tg_chat, "‚úÖ Clip sent to group. You can stop the camera stream in the app now.")

        # Sau khi ƒë√£ g·ª≠i video, kh√¥ng detect n·ªØa, ch·ªâ tr·∫£ frame
        return av.VideoFrame.from_ndarray(img, format="bgr24")

    webrtc_streamer(
        key="camera_fall_detector",
        video_frame_callback=video_callback,
        media_stream_constraints={"video": True, "audio": False}
    )